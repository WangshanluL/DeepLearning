# Attention机制完整计算过程详解

## 核心公式

```
Attention(Q,K,V) = softmax(QK^T/√d_k)V
```

## 实际例子：翻译"我爱猫"

### 设置参数

- 序列长度：3个词 ["我", "爱", "猫"]
- 词嵌入维度：4
- 注意力头维度：4 (d_k = 4)

### 步骤1：输入词嵌入矩阵X (3×4)

```
X = [[ 1.0,  0.5,  0.2,  0.1],    # "我"的嵌入向量
     [ 0.3,  1.2,  0.8,  0.4],    # "爱"的嵌入向量  
     [ 0.6,  0.1,  1.5,  0.9]]    # "猫"的嵌入向量
```

### 步骤2：定义权重矩阵

```
W_Q = [[ 1.0,  0.0,  0.5,  0.2],    # Query权重矩阵 (4×4)
       [ 0.1,  1.0,  0.3,  0.1],
       [ 0.2,  0.1,  1.0,  0.4],
       [ 0.0,  0.2,  0.1,  1.0]]

W_K = [[ 0.8,  0.1,  0.3,  0.2],    # Key权重矩阵 (4×4)
       [ 0.2,  0.9,  0.1,  0.3],
       [ 0.1,  0.2,  1.1,  0.1],
       [ 0.3,  0.1,  0.2,  0.8]]

W_V = [[ 1.1,  0.2,  0.1,  0.3],    # Value权重矩阵 (4×4)
       [ 0.1,  1.0,  0.2,  0.4],
       [ 0.3,  0.1,  1.2,  0.2],
       [ 0.2,  0.3,  0.1,  1.0]]
```

### 步骤3：计算Q、K、V矩阵

**计算Q = X × W_Q (3×4)：**

```
Q[0] = X[0] × W_Q = [1.0, 0.5, 0.2, 0.1] × W_Q = [1.14, 0.57, 1.04, 0.31]
Q[1] = X[1] × W_Q = [0.3, 1.2, 0.8, 0.4] × W_Q = [0.51, 1.58, 1.43, 0.70]
Q[2] = X[2] × W_Q = [0.6, 0.1, 1.5, 0.9] × W_Q = [0.90, 0.29, 2.04, 1.22]

Q = [[1.14, 0.57, 1.04, 0.31],    # "我"的Query向量
     [0.51, 1.58, 1.43, 0.70],    # "爱"的Query向量
     [0.90, 0.29, 2.04, 1.22]]    # "猫"的Query向量
```

**计算K = X × W_K (3×4)：**

```
K[0] = X[0] × W_K = [1.0, 0.5, 0.2, 0.1] × W_K = [0.93, 0.58, 0.55, 0.31]
K[1] = X[1] × W_K = [0.3, 1.2, 0.8, 0.4] × W_K = [0.54, 1.28, 1.21, 0.70]
K[2] = X[2] × W_K = [0.6, 0.1, 1.5, 0.9] × W_K = [0.87, 0.25, 1.89, 0.87]

K = [[0.93, 0.58, 0.55, 0.31],    # "我"的Key向量
     [0.54, 1.28, 1.21, 0.70],    # "爱"的Key向量
     [0.87, 0.25, 1.89, 0.87]]    # "猫"的Key向量
```

**计算V = X × W_V (3×4)：**

```
V[0] = X[0] × W_V = [1.0, 0.5, 0.2, 0.1] × W_V = [1.23, 0.40, 0.37, 0.53]
V[1] = X[1] × W_V = [0.3, 1.2, 0.8, 0.4] × W_V = [0.87, 1.61, 1.22, 0.94]
V[2] = X[2] × W_V = [0.6, 0.1, 1.5, 0.9] × W_V = [1.29, 0.52, 2.07, 1.18]

V = [[1.23, 0.40, 0.37, 0.53],    # "我"的Value向量
     [0.87, 1.61, 1.22, 0.94],    # "爱"的Value向量
     [1.29, 0.52, 2.07, 1.18]]    # "猫"的Value向量
```

### 步骤4：计算注意力分数 QK^T

**计算QK^T (3×3)：**

```
QK^T[0,0] = Q[0] · K[0] = [1.14,0.57,1.04,0.31] · [0.93,0.58,0.55,0.31] 
          = 1.14×0.93 + 0.57×0.58 + 1.04×0.55 + 0.31×0.31 = 2.29

QK^T[0,1] = Q[0] · K[1] = [1.14,0.57,1.04,0.31] · [0.54,1.28,1.21,0.70]
          = 1.14×0.54 + 0.57×1.28 + 1.04×1.21 + 0.31×0.70 = 2.69

QK^T[0,2] = Q[0] · K[2] = [1.14,0.57,1.04,0.31] · [0.87,0.25,1.89,0.87]
          = 1.14×0.87 + 0.57×0.25 + 1.04×1.89 + 0.31×0.87 = 3.41

类似地计算其他元素：

QK^T = [[2.29, 2.69, 3.41],    # "我"对所有词的原始注意力分数
        [2.05, 3.84, 4.12],    # "爱"对所有词的原始注意力分数  
        [2.48, 4.69, 6.25]]    # "猫"对所有词的原始注意力分数
```

### 步骤5：缩放 (除以√d_k)

```
d_k = 4，所以 √d_k = 2

Scaled = QK^T / √d_k = [[1.15, 1.35, 1.71],
                        [1.03, 1.92, 2.06],
                        [1.24, 2.35, 3.13]]
```

### 步骤6：应用Softmax

对每一行分别应用softmax函数：

**第1行 (我)：**

```
输入: [1.15, 1.35, 1.71]
exp值: [3.16, 3.86, 5.53]
总和: 3.16 + 3.86 + 5.53 = 12.55
softmax: [3.16/12.55, 3.86/12.55, 5.53/12.55] = [0.25, 0.31, 0.44]
```

**第2行 (爱)：**

```
输入: [1.03, 1.92, 2.06] 
exp值: [2.80, 6.82, 7.85]
总和: 17.47
softmax: [0.16, 0.39, 0.45]
```

**第3行 (猫)：**

```
输入: [1.24, 2.35, 3.13]
exp值: [3.46, 10.48, 22.81] 
总和: 36.75
softmax: [0.09, 0.29, 0.62]
```

**注意力权重矩阵：**

```
Attention_weights = [[0.25, 0.31, 0.44],    # "我"的注意力分布
                     [0.16, 0.39, 0.45],    # "爱"的注意力分布
                     [0.09, 0.29, 0.62]]    # "猫"的注意力分布
```

### 步骤7：计算最终输出 (Attention_weights × V)

**"我"的输出：**

```
Output[0] = 0.25×V[0] + 0.31×V[1] + 0.44×V[2]
          = 0.25×[1.23,0.40,0.37,0.53] + 0.31×[0.87,1.61,1.22,0.94] + 0.44×[1.29,0.52,2.07,1.18]
          = [0.31,0.10,0.09,0.13] + [0.27,0.50,0.38,0.29] + [0.57,0.23,0.91,0.52]
          = [1.15, 0.83, 1.38, 0.94]
```

**"爱"的输出：**

```
Output[1] = 0.16×V[0] + 0.39×V[1] + 0.45×V[2] = [1.10, 1.09, 1.68, 1.07]
```

**"猫"的输出：**

```
Output[2] = 0.09×V[0] + 0.29×V[1] + 0.62×V[2] = [1.17, 0.82, 1.64, 1.06]
```

### 最终输出矩阵：

```
Output = [[1.15, 0.83, 1.38, 0.94],    # 经过attention后"我"的表示
          [1.10, 1.09, 1.68, 1.07],    # 经过attention后"爱"的表示  
          [1.17, 0.82, 1.64, 1.06]]    # 经过attention后"猫"的表示
```

## 结果解释

从注意力权重可以看出：

- "我" 最关注 "猫"(0.44) > "爱"(0.31) > "我"(0.25)
- "爱" 最关注 "猫"(0.45) > "爱"(0.39) > "我"(0.16)
- "猫" 最关注 "猫"(0.62) > "爱"(0.29) > "我"(0.09)

这说明在这个句子中，每个词都倾向于关注"猫"这个重要的名词！

## 完整公式验证

```
Attention(Q,K,V) = softmax(QK^T/√d_k)V ✓
```

我们成功地按照这个公式完成了所有计算步骤！